---
layout:     post
title:      "「机器学习」 图神经网络"
subtitle:   "Graph Neural Network"
date:       2023-03-16 12:00:00
author:     "Azaan"
header-img: "img/post-bg-machine-learning.jpg"
katex: true
tags:
    - 机器学习
---





# 图简介

图是一种非常通用的结构，由一组节点组成，其中节点通过边来连接。现实世界中的一些对象可以自然地采用图的形式，例如道路网络可以被视为图，其中节点是物理位置，边代表它们之间的道路；化学分子是一种小的图，其中节点代表原子，边代表化学键。此外，许多数据集也可以用图来表示，例如，社交网络是一种图，其中节点是人，边缘代表它们之间的社交关系；科学文献可以看作一个图，其中节点是论文，边表示涉及这些变量的计算。

此外，集合可以被视为一个图，其中每个成员都是一个节点，并相互连接。图像可以被视为具有规则拓扑的图，其中每个像素都是一个节点，其边缘指向八个相邻像素。

### 图的类型

![](https://azaan-zheng.github.io/img/machine-learning/20230316/1.jpg)

图可以通过多种方式进行分类。a）中的社交关系中包含的是无向边，这种关系没有方向性；而b）中的引文网络包含有向边，论文之间的引用关系本质上是单向的。c）描述了一个知识图，它通过定义对象之间的关系来编码关于对象的一组事实，这是一个有向异构多重图。它是异构的，因为节点可以代表不同类型的实体（如人、国家、公司），它又是一个多重图，因为任何两个节点之间都可以有不同类型的多条边。通过将每个点连接到其最近的邻居，可以将d）中的表示飞机的点云转换为图。e）代表了一个层次图，桌子、灯和房间分别由表示其各自组件的相邻性的图形来描述，这三个图本身又是另一个图中的节点，该图表示更大的对象的拓扑。

所有类型的图都可以使用深度学习进行处理，我们重点探讨的是无向图。

### 图的表示

![](https://azaan-zheng.github.io/img/machine-learning/20230316/2.jpg)

一个图由 $N$ 个节点和 $E$ 条边组成，它可以由三个矩阵 $\mathbf{A,X,E}$ 表示。其中 $\mathbf{A}$ 表示节点之间的邻接矩阵（$N \times N$），即如果点 $m$ 和点 $n$ 之间有一条边邻接，那么 $\mathbf{A}[m,n] = 1$。$\mathbf{X}$ 表示节点的 $embedding$，大小为 $D \times N$，其中的第 $n$ 个列向量 $\mathbf{x^{(n)}}$ 即表示第 $n$ 个节点的长度为 $D$ 的 $embedding$ 。类似的，$\mathbf{E}$ 表示边的 $embedding$，大小为 $D_e \times N$，其中的第 $e$ 个列向量 $\mathbf{e^{(n)}}$ 即表示第 $e$ 条边的长度为 $D_e$ 的 $embedding$ 。

为了简单起见，我们后面首先考虑只有节点 $embedding$ 的图。

### 节点索引置换

图的节点其实并没有一个严格的顺序，这意味着我们可以用不同的方式为图节点选择索引，而索引不同的情况下，图的表示矩阵也就会发生变化。换言之，我们通过选择不同索引的方式，即使改变了图的表示矩阵，却没有改变图表示的含义，这一点和处理图像时的像素点矩阵大不相同。

我们可以通过引入置换矩阵 $\mathbf{P}$ 来表示这一性质。置换矩阵的每一列（或每一行）都有且仅有一个位置是1而其余位置都是0，当 $\mathbf{P}[m,n]=1$ 时，这表示前一个图的第 $m$ 个节点通过变换之后将成为新图中的第 $n$ 个节点。以下公式揭露了使用置换矩阵生成新图的关系：


$$
\mathbf{X'=XP} \\
\mathbf{A'=P A P^T}
$$


其中 $\mathbf{A'}$ 的置换可以这样理解，我们使用一个独热向量 $\mathbf{v_n}$，它表示第 $n$ 个节点，它的第 $n$ 个位置为1而其余位置为0。我们可知 $\mathbf{A'{v_n}'}$ 即表示了节点 $n$ 在新图中的邻接情况。而从另一个方面来说，我们可以通过 $\mathbf{P^T v_n'=v_n}$ 的到原图的独热向量，并通过左乘 $\mathbf{A}$ 得到原图中的临界情况，再推广到新图。其实由于置换矩阵具备性质 $\mathbf{P^T = P^{-1}}$ ，因此这一过程可以通过过渡矩阵和相似矩阵的关系来理解。 

# 图神经网络问题分类

图神经网络将节点 $embedding$ 矩阵 $\mathbf{X}$ 和邻接矩阵 $\mathbf{A}$ 作为输入，并使其通过 $K$ 层，在每一层都将更新节点 $embedding$ 以创建中间的隐藏表示 $\mathbf{H}_k$，并最终计算输出的 $\mathbf{H}_K$。在网络起始部分，输入节点 $embedding$ 只包含关于节点自身的信息，最后模型输出的 $\mathbf{H}_K$ 的每一列都包括关于图中节点及其上下文的信息。这类似于 transformer 中的单词 $embedding$，在开始只表示单词，但在最后表示句子上下文中的单词含义。

我们首先描述图神经网络处理的问题类型以及每种问题相关的损失函数。

![](https://azaan-zheng.github.io/img/machine-learning/20230316/3.jpg)

### 图层面任务





### 节点层面任务





### 边预测任务













### 参考文献

1. 《Understanding Deep Learning》Simon J.D. Prince



